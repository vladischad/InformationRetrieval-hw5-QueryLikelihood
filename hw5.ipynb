{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edba0e03-8159-429a-b88a-b03fb3a973d8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25c41159fbc3b698",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 5 - Retrieval Models: Query Likelihood\n",
    "CS 437  \n",
    "Fall 2025  \n",
    "Dr. Henderson  \n",
    "_v1_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f67ad-7e68-4fed-aba3-ad8c97f67084",
   "metadata": {},
   "source": [
    "**First copy your `hw4lib.py` from the previous assignment as `hw5lib.py` in the same directory as this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16b75b-24a4-4374-a089-4a7ee0e0f6b7",
   "metadata": {},
   "source": [
    "These imports will be useful for the following exercises. You may add other imports here if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec50cb6-08dd-483b-8fb8-b53c85f8dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import cacmlib\n",
    "import hw5lib as hw5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e72109-bdd3-44d6-a288-b559e81ca531",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adc871-5357-4eaa-955f-9386778a5031",
   "metadata": {},
   "source": [
    "You will be doing most of your development in the `hw5lib.py` module and then running unit tests and other code from this notebook. The `importlib` module has been included above so that you can reload your `hw5lib.py` changes without having to restart the kernel. This makes iterating much faster but can sometimes lead to confusing states of the notebook (if you don't understand Python imports). If you get stuck on a bug it's a good idea to reset the kernel to rule out any reloading issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777af4f3-63db-45a3-ae3d-3d312d78b5ca",
   "metadata": {},
   "source": [
    "_Run this cell anytime you make changes to `hw5lib.py` to update the current state of your notebook_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a800f7-1cd3-4c4e-96b9-47cfbf72b26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'hw5lib' from '/home/ehenders/VOL/CLASSES/CS437/homework/source/hw5/hw5lib.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(hw5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36322dbd-291d-4ab7-94ee-338b7ad811a1",
   "metadata": {},
   "source": [
    "## 1. Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab868e16-758e-428d-9299-15f4a7d25e54",
   "metadata": {},
   "source": [
    "The first model we studied is based on representing documents and queries as weighted vectors of terms and ranking documents based on a similarity measure (e.g. cosine distance). For this assignment, you will build on your previous work to build a a vector space ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9bc54-793e-41e4-a5ba-17f6a0860490",
   "metadata": {},
   "source": [
    "Run the cell below to initiate your database and add the documents in the `tests` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0279b897-ac32-4c83-95e1-9c39ea1a9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.create_db()\n",
    "assert os.path.isfile('index.db')\n",
    "\n",
    "hw5.add_docs('tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0349c6-a309-41f8-8f23-cf62a048f512",
   "metadata": {},
   "source": [
    "The equation derived in the book that we will use for document vector term weights is:  \n",
    "\n",
    "#### $ w_{ik} = \\frac{(\\log(f_{ik}) + 1) \\cdot \\log(N/n_k) }{ \\sqrt{ \\sum\\limits_k \\left[ (\\log(f_{ik}) + 1) \\cdot \\log(N/n_k)\\right]^2 } } $\n",
    "\n",
    "$ f_{ik} $ is the frequency of the term $ k $ in the documentd $ d_i $  \n",
    "$ N $ is the number of documents  \n",
    "$ n_k $ is the number of documents the term $ k $ appears in\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b7240-0ee2-4e7b-a484-0a093d7f2d32",
   "metadata": {},
   "source": [
    "1.1 (55 pts) Create a function named `index_vector_model()` in the `hw5lib.py` file that takes no parameters. Your function should read the current list of documents in the `index.db` database and for each document it should compute the vector weights for each term (use standard preprocssing to filter terms) and store them in a new table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5caf85f2-6bb1-455a-a6ca-ca7ebe7364a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.index_vector_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292897c-ee4f-4895-8bbe-3f9960c45ae5",
   "metadata": {},
   "source": [
    "1.2 (15 pts) Create a function named `get_document_vector()` in the `hw5lib.py` file that takes a single string filename and returns the associated weighted terms of the document vector as a sorted list (in decreasing weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78375d92-d5e2-447f-8f62-867aa160ea73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = hw5.get_document_vector('tests/doc2.txt')[:5]\n",
    "assert results == [('queri', 0.369569331014455), ('databas', 0.320178350159223), ('store', 0.21827360034479323), ('retriev', 0.21827360034479323), ('sql', 0.21827360034479323)], results\n",
    "results = hw5.get_document_vector('tests/doc7.txt')[:5]\n",
    "assert results  == [('analyt', 0.394498512110325), ('extract', 0.23299717628792277), ('insight', 0.23299717628792277), ('statist', 0.23299717628792277), ('analysi', 0.23299717628792277)], results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e094e-7b42-496c-ab39-b88fd634b912",
   "metadata": {},
   "source": [
    "To rank documents requires computing a document vector for the query and then using the cosine similarity measure to produce a similarity score that can be used for ranking. The term weight equation is the same as above, except the terms to use, i.e. $ f_{ik} $, are taken from the query, not the document. To compute the cosine similarity, simply take the dot product of the query vector and the document vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff4f70-fad6-45ab-a4b8-d55a91544a3d",
   "metadata": {},
   "source": [
    "1.3 (25 pts) Create a function named `rank_document_model()` in the `hw5lib.py` file that takes a query string and returns a ranked list of relevant documents as a tuple `(score, filename)`. Your function should compute the term weights of the query vector and then use the cosine similarity measure to rank all the documents in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24990d4f-ed77-44d0-9aea-26954a362190",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hw5.rank_document_model(\"machine learning\")\n",
    "assert results == [(0.25006297536920996, 'tests/doc9.txt'), (0.24903727223083577, 'tests/doc1.txt'), (0.07552280941097185, 'tests/doc5.txt')], results\n",
    "results = hw5.rank_document_model(\"database transactions\")\n",
    "assert results == [(0.3807430255445131, 'tests/doc2.txt'), (0.10723037781462168, 'tests/doc3.txt')], results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda4d5e-7ead-4c8a-a771-a75b7d38e4e2",
   "metadata": {},
   "source": [
    "1.4 (5 pts) Create a function named `query_document_model()` which takes a query string and returns a ranked list of relevant documents as a list of filename strings. This is just a wrapper around the previous function so it can be used in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9130702a-b242-4b48-b453-0b72f1798845",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hw5.query_document_model(\"machine learning\")\n",
    "assert results == ['tests/doc9.txt', 'tests/doc1.txt', 'tests/doc5.txt'], results\n",
    "results = hw5.query_document_model(\"database transactions\")\n",
    "assert results == ['tests/doc2.txt', 'tests/doc3.txt'], results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0f118-b278-4a28-9881-992cf0189fc7",
   "metadata": {},
   "source": [
    "# 2. Query Likelihood Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba35dfb-ea45-4435-9066-369e59a868ee",
   "metadata": {},
   "source": [
    "We looked at several algorithms based on language models. For this assignment you will implement the _Query Likelihood Model_, which computes the probability of the query $ Q $ being generated from the language model of the document $ D $ using the equation:  \n",
    "#### $ \\log P(Q|D) = \\sum\\limits_n \\log p(q_n|D) $  \n",
    "\n",
    "### $ p(q_n|D) = \\frac{f_{q_n,D} + \\mu \\frac{c_{q_n}}{|C|}}{|D| + \\mu} $  \n",
    "\n",
    "$n$ is used to index each of the query terms, and $ c_{q_n} $ is the count of query term $q_n$ in the entire document collection. $|C|$ is the count of all term occurrences in the collection and $ |D| $ is the count of all term occurrences in the document $ D $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48d318-3cb6-4428-8b61-c8b9b345e8a4",
   "metadata": {},
   "source": [
    "2.1 (55 pts) Create a function named `index_query_likelihood()` in the `hw5lib.py` file that takes no parameters. Your function should read the current list of documents in the `index.db` database and for each document it should estimate the log probability $ p(q_n|D) $ for each term and store it in a new table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "199e1ec0-b24b-4948-87ae-a708b347ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.index_query_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9b1cd-7c03-4c8e-a002-4d47759798b1",
   "metadata": {},
   "source": [
    "2.2 (15 pts) Create a function named `get_document_likelihood()` in the `hw5lib.py` file that takes a single string filename and returns the associated term likelihoods of the document as a sorted list (in decreasing likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35743885-1a9b-4a53-a46e-6d19fd1f794e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = hw5.get_document_likelihood('tests/doc2.txt')[:5]\n",
    "assert results == [('data', 0.002159217414503846), ('system', 0.001956289443551291), ('process', 0.0018175973156571504), ('databas', 0.0016591423226115568), ('improv', 0.0016418316588775997)], results\n",
    "results = hw5.get_document_likelihood('tests/doc7.txt')[:5]\n",
    "assert results  ==  [('data', 0.002172469687489766), ('system', 0.001957254083316356), ('process', 0.0018306811369178007), ('larg', 0.0014888925863458355), ('platform', 0.0014888925863458355)], results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ad600-28f4-4bb3-8cd4-359dc18cdc79",
   "metadata": {},
   "source": [
    "To rank documents requires computing a score for each document by summing all the log probabilities for terms in the document that appear in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964e3e0-4e4b-4abd-a872-611f3bfe8160",
   "metadata": {},
   "source": [
    "2.3 (25 pts) Create a function named `rank_query_likelihood()` in the `hw5lib.py` file that takes a query string and returns a ranked list of relevant documents as a tuple `(score, filename)`. Your function should compute the query likelihood of each document and use it to rank all the documents in the database.  \n",
    "_IMPORTANT: Try to optimize your database calls or the evaluation in section 3 below could take a very long time._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d158d5-c144-46c9-834c-1c8b98398206",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hw5.rank_query_likelihood(\"machine learning\")\n",
    "assert results == [(0.003125369390242128, 'tests/doc1.txt'), (0.003123832071309397, 'tests/doc9.txt'), (0.001485961695427832, 'tests/doc5.txt')], results\n",
    "results = hw5.rank_query_likelihood(\"database transactions\")\n",
    "assert results == [(0.0026517676772958424, 'tests/doc2.txt'), (0.0016230908384346938, 'tests/doc3.txt')], results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdf7f0-ce0b-4617-a7b6-14aa213d2616",
   "metadata": {},
   "source": [
    "2.4 (5 pts) Create a function named `query_likelihood()` which takes a query string and returns a ranked list of relevant documents as a list of filename strings. This is just a wrapper around the previous function so it can be used in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e10ef34b-e36f-43e5-8496-9705839c2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hw5.query_likelihood(\"machine learning\")\n",
    "assert results == ['tests/doc1.txt', 'tests/doc9.txt', 'tests/doc5.txt'], results\n",
    "results = hw5.query_likelihood(\"database transactions\")\n",
    "assert results == ['tests/doc2.txt', 'tests/doc3.txt'], results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98faad2b-0b69-4970-b3ce-d238bdd64036",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e5b9b-faca-49fe-8a32-7bff357b364e",
   "metadata": {},
   "source": [
    "We will use the _CACM_ dataset to evaluate your search engine. Indexing the entire _CACM_ dataset can take several minutes. The following cell is commented out so that id does not run during grading -- instead you should run it manually (by uncommenting the code) once you are finished with your implementation. You will include your database as part of your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d53340-7ab9-4ab2-8614-c6692449fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.create_db()\n",
    "hw5.add_docs('../../datasets/cacm/docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e941cf-be63-49e5-9edd-d49387dc25cb",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Please remember to recomment the code in the cell above after you have built your index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a4544-383b-4a6e-ae3b-d115e823268b",
   "metadata": {},
   "source": [
    "3.1 (10pts) Index the _CACM_ dataset using your vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff381d4-0cab-4c85-9703-45f75f1b27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.index_vector_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eb5a3-eeb9-44d4-b6b4-2a5799f185be",
   "metadata": {},
   "source": [
    "Run the cell below to get the document vector for `CACM-0002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0db79-ff62-48ce-b77a-0144bae79018",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.get_document_vector('../../datasets/cacm/docs/CACM-0002.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1d962-f8b5-4e33-a437-aba65c0ea011",
   "metadata": {},
   "source": [
    "3.2 (5 pts) Give a brief analysis of the results -- are they expected and what changes if any would you make to your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83004e-67f6-4146-a9b0-f7a6399e3360",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d18cc8-9b11-4dee-a7ba-1915b0018a30",
   "metadata": {},
   "source": [
    "3.3 (5 pts) Evaluate your implementation using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec732137-e42e-46b4-8e26-732ebc5b626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cacmlib)\n",
    "cacmlib.eval(hw5.query_document_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f608dcf-e7d5-418e-838c-4faa17009184",
   "metadata": {},
   "source": [
    "3.4 (10 pts) Index the _CACM_ dataset using your query likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bcb88a-e361-4a5c-87a2-c3499b840d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.index_query_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e4952-f542-4afc-a1b1-5542ad4ed41d",
   "metadata": {},
   "source": [
    "Run the cell below to get the document likelihood for `CACM-0002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824c2f0-b870-48b7-a138-c8c8df88f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw5.get_document_likelihood('../../datasets/cacm/docs/CACM-0002.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770c4b0-bd79-4f53-827b-7fd38487d4b5",
   "metadata": {},
   "source": [
    "3.5 (5 pts) How does it compare to your results from 3.2 and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78524ed3-a0aa-42f7-990f-cdd2e0f5e8f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd028f06-99a1-4daf-9da7-b307c0415540",
   "metadata": {},
   "source": [
    "3.6 (5 pts) Evaluate your implementation using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd301a-6a5f-42f4-8743-b1295a5caee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacmlib.eval(hw5.query_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f382fd6-2e80-4bc9-bb42-1d31d9209dac",
   "metadata": {},
   "source": [
    "3.7 (5 pts) Compare the results to the vector space model and give possible explanations for the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa259e-001d-4ffd-880e-0b21623c68f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0968c5c-7ff6-42e0-a31f-18acd05214fc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69f6d8ebed4ba908",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f1b34-912c-4ee7-b859-c58fe85862ef",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bc6d0417687cb84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Be sure to ***SAVE YOUR WORK***!  \n",
    "\n",
    "Next, select Kernel -> Restart Kernel and Run All Cells...\n",
    "\n",
    "Make sure there are no errors in **Section 1**.\n",
    "\n",
    "Then uncomment the first cell in **Section 2** and manually run the cells in **Section 2**.\n",
    "\n",
    "Make sure there are no errors in **Section 2**.\n",
    "\n",
    "***IMPORTANT***: recomment the code in the first cell in **Section 2** again.\n",
    "\n",
    "Then submit the following to Canvas:\n",
    "\n",
    "This notebook (`.ipynb`) file using the original name - do not rename it!\n",
    "\n",
    "Your `hw5lib.py` file with your implementation.\n",
    "\n",
    "Your `index.db` file containing the index of the _CACM_ directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
